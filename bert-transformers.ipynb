{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install transfomers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"I know that since the inception of the 20-over format a decade and a half back, and that of the Indian Premier League (IPL) 4 years after that, the shortest form of the game is the most popular one. It has also done wonders for shining the spotlight on the upcoming talent from various countries. And finally, the League format has also blurred the national boundaries by having teams consisting of players from various nationalities. But I think that is the end of “everything good about T20 cricket and the IPL”.\n",
    "\n",
    "I watched the first 2 seasons of this cricket extravaganza, but I simply can’t bear it now.\n",
    "\n",
    "It no longer feels like Cricket. It doesn’t even feel like a real sport anymore. There is no battle of wits. No contest of talent. No display of true strategic genius. It’s all a show. Entertainment. The IPL is to cricket what WWE is to wrestling - loud, methodical, and formulaic.\n",
    "\n",
    "It’s a batsmen’s orgy. A ludicrous display of powerful, mechanical shots. A 3 hour long slugfest of who can dispatch the longest boundary - no skill, no finesse, just pure muscle.\n",
    "\n",
    "The crowd is so dead that they need to replay that annoying IPL song every two overs. And then blast senseless Bollywood beats to punctuate the silence in the stadium. Finally, there is that annoying RJ person who has to ruin it beyond salvation with their cheeky comments, leading chants, and overall irritating persona.\n",
    "\n",
    "I mean if the game is good, you wouldn’t need all these crutches. You don’t need a booming loudspeaker voice to start a “Dhoni Dhoni” chant. Sachin never needed that. Neither did Sehwag.\n",
    "\n",
    "And then comes the endless corporate greed of the BCCI. You have your ticket sales, your advertisements, revenue from the stadia and the local cricket bodies, the sponsorships from various companies who sponsor the various post-match and post-series awards, and the endless numbers of endorsements on your players’ apparel, branded billboards, cricket gears, and boundaries. But even that isn’t enough. Everything has been sold. A shot that clears the boundary rope without ever making contact is no longer a six, it is a “Yes Bank Maximum”. Fans aren’t fans, they are the “Vodafone fan army”. Beautiful, successful catches deserve no more adjectives than “Karbonn kamaal”.\n",
    "\n",
    "But even if you ignore the cringe inducing, invasive corporate endorsements thinly veiled as commentary, that particular art form is still in the last throes of its organic life.\n",
    "\n",
    "I was just watching the Pune vs Hyderabad match and Dhoni was once more the steady anchor for his team. He was trying, as he does, to balance both aggression and equanimity (and doing, as expected, a stellar job). The commentator were trying to analyze his form, his style, and trying to decipher what is going on in that shrewd mind of his. But all they could muster was “he is such a strong player”, “Dhoni has such strong forearms”, and “his upper body strength is incredible”. That’s just lazy, boring, and frankly offensive to the fans. Everyone knows that. Just like everyone knows that the chasing team needs to score runs and save wickets to win. Or that one disastrous over could spell the doom for the defending side.\n",
    "\n",
    "Where are the humorous quips? Quick nuggets of cricketing strategy? Stories about the game and the players? Don’t tell me that it was a “well deserved century”; elaborate on why it was. Don’t call every six “magnificent”, or every clean bowled delivery a “stunner”. Don’t read out the statistics that are on the screen for everyone to read. Go read about this sport that is now a phenomenon. Know its history, its science, its literature.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I know that since the inception of the 20-over format a decade and a half back, and that of the Indian Premier League (IPL) 4 years after that, the shortest form of the game is the most popular one. It has also done wonders for shining the spotlight on the upcoming talent from various countries. And finally, the League format has also blurred the national boundaries by having teams consisting of players from various nationalities. But I think that is the end of “everything good about T20 cricket and the IPL”.\\n\\nI watched the first 2 seasons of this cricket extravaganza, but I simply can’t bear it now.\\n\\nIt no longer feels like Cricket. It doesn’t even feel like a real sport anymore. There is no battle of wits. No contest of talent. No display of true strategic genius. It’s all a show. Entertainment. The IPL is to cricket what WWE is to wrestling - loud, methodical, and formulaic.\\n\\nIt’s a batsmen’s orgy. A ludicrous display of powerful, mechanical shots. A 3 hour long slugfest of who can dispatch the longest boundary - no skill, no finesse, just pure muscle.\\n\\nThe crowd is so dead that they need to replay that annoying IPL song every two overs. And then blast senseless Bollywood beats to punctuate the silence in the stadium. Finally, there is that annoying RJ person who has to ruin it beyond salvation with their cheeky comments, leading chants, and overall irritating persona.\\n\\nI mean if the game is good, you wouldn’t need all these crutches. You don’t need a booming loudspeaker voice to start a “Dhoni Dhoni” chant. Sachin never needed that. Neither did Sehwag.\\n\\nAnd then comes the endless corporate greed of the BCCI. You have your ticket sales, your advertisements, revenue from the stadia and the local cricket bodies, the sponsorships from various companies who sponsor the various post-match and post-series awards, and the endless numbers of endorsements on your players’ apparel, branded billboards, cricket gears, and boundaries. But even that isn’t enough. Everything has been sold. A shot that clears the boundary rope without ever making contact is no longer a six, it is a “Yes Bank Maximum”. Fans aren’t fans, they are the “Vodafone fan army”. Beautiful, successful catches deserve no more adjectives than “Karbonn kamaal”.\\n\\nBut even if you ignore the cringe inducing, invasive corporate endorsements thinly veiled as commentary, that particular art form is still in the last throes of its organic life.\\n\\nI was just watching the Pune vs Hyderabad match and Dhoni was once more the steady anchor for his team. He was trying, as he does, to balance both aggression and equanimity (and doing, as expected, a stellar job). The commentator were trying to analyze his form, his style, and trying to decipher what is going on in that shrewd mind of his. But all they could muster was “he is such a strong player”, “Dhoni has such strong forearms”, and “his upper body strength is incredible”. That’s just lazy, boring, and frankly offensive to the fans. Everyone knows that. Just like everyone knows that the chasing team needs to score runs and save wickets to win. Or that one disastrous over could spell the doom for the defending side.\\n\\nWhere are the humorous quips? Quick nuggets of cricketing strategy? Stories about the game and the players? Don’t tell me that it was a “well deserved century”; elaborate on why it was. Don’t call every six “magnificent”, or every clean bowled delivery a “stunner”. Don’t read out the statistics that are on the screen for everyone to read. Go read about this sport that is now a phenomenon. Know its history, its science, its literature.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=tokenizer(text,return_tensors=\"pt\",truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045,  2113,  2008,  2144,  1996, 12149,  1997,  1996,  2322,\n",
       "          1011,  2058,  4289,  1037,  5476,  1998,  1037,  2431,  2067,  1010,\n",
       "          1998,  2008,  1997,  1996,  2796,  4239,  2223,  1006, 12997,  2140,\n",
       "          1007,  1018,  2086,  2044,  2008,  1010,  1996, 20047,  2433,  1997,\n",
       "          1996,  2208,  2003,  1996,  2087,  2759,  2028,  1012,  2009,  2038,\n",
       "          2036,  2589, 16278,  2005,  9716,  1996, 17763,  2006,  1996,  9046,\n",
       "          5848,  2013,  2536,  3032,  1012,  1998,  2633,  1010,  1996,  2223,\n",
       "          4289,  2038,  2036, 18449,  1996,  2120,  7372,  2011,  2383,  2780,\n",
       "          5398,  1997,  2867,  2013,  2536, 24904,  1012,  2021,  1045,  2228,\n",
       "          2008,  2003,  1996,  2203,  1997,  1523,  2673,  2204,  2055,  1056,\n",
       "         11387,  4533,  1998,  1996, 12997,  2140,  1524,  1012,  1045,  3427,\n",
       "          1996,  2034,  1016,  3692,  1997,  2023,  4533,  4469,  3567,  5289,\n",
       "          4143,  1010,  2021,  1045,  3432,  2064,  1521,  1056,  4562,  2009,\n",
       "          2085,  1012,  2009,  2053,  2936,  5683,  2066,  4533,  1012,  2009,\n",
       "          2987,  1521,  1056,  2130,  2514,  2066,  1037,  2613,  4368,  4902,\n",
       "          1012,  2045,  2003,  2053,  2645,  1997, 25433,  1012,  2053,  5049,\n",
       "          1997,  5848,  1012,  2053,  4653,  1997,  2995,  6143, 11067,  1012,\n",
       "          2009,  1521,  1055,  2035,  1037,  2265,  1012,  4024,  1012,  1996,\n",
       "         12997,  2140,  2003,  2000,  4533,  2054, 11700,  2003,  2000,  4843,\n",
       "          1011,  5189,  1010,  4118,  7476,  1010,  1998,  5675,  2594,  1012,\n",
       "          2009,  1521,  1055,  1037, 12236,  3549,  1521,  1055,  8917,  2100,\n",
       "          1012,  1037, 11320, 14808, 13288,  4653,  1997,  3928,  1010,  6228,\n",
       "          7171,  1012,  1037,  1017,  3178,  2146, 23667, 14081,  1997,  2040,\n",
       "          2064, 18365,  1996,  6493,  6192,  1011,  2053,  8066,  1010,  2053,\n",
       "         21892,  3366,  1010,  2074,  5760,  6740,  1012,  1996,  4306,  2003,\n",
       "          2061,  2757,  2008,  2027,  2342,  2000, 15712,  2008, 15703, 12997,\n",
       "          2140,  2299,  2296,  2048, 15849,  1012,  1998,  2059,  8479,  3168,\n",
       "          3238, 16046, 10299,  2000, 26136,  6593, 20598,  1996,  4223,  1999,\n",
       "          1996,  3346,  1012,  2633,  1010,  2045,  2003,  2008, 15703,  1054,\n",
       "          3501,  2711,  2040,  2038,  2000, 10083,  2009,  3458, 12611,  2007,\n",
       "          2037,  5048,  2100,  7928,  1010,  2877, 29534,  1010,  1998,  3452,\n",
       "         29348, 16115,  1012,  1045,  2812,  2065,  1996,  2208,  2003,  2204,\n",
       "          1010,  2017,  2876,  1521,  1056,  2342,  2035,  2122, 13675,  4904,\n",
       "          8376,  1012,  2017,  2123,  1521,  1056,  2342,  1037, 24716,  5189,\n",
       "         13102, 25508,  2121,  2376,  2000,  2707,  1037,  1523, 28144, 10698,\n",
       "         28144, 10698,  1524, 16883,  1012, 17266, 10606,  2196,  2734,  2008,\n",
       "          1012,  4445,  2106,  7367, 18663,  2290,  1012,  1998,  2059,  3310,\n",
       "          1996, 10866,  5971, 22040,  1997,  1996,  4647,  6895,  1012,  2017,\n",
       "          2031,  2115,  7281,  4341,  1010,  2115, 14389,  1010,  6599,  2013,\n",
       "          1996,  2358, 25205,  1998,  1996,  2334,  4533,  4230,  1010,  1996,\n",
       "         12026,  2015,  2013,  2536,  3316,  2040, 10460,  1996,  2536,  2695,\n",
       "          1011,  2674,  1998,  2695,  1011,  2186,  2982,  1010,  1998,  1996,\n",
       "         10866,  3616,  1997, 20380,  2015,  2006,  2115,  2867,  1521, 26278,\n",
       "          1010, 11180,  4908,  2015,  1010,  4533, 19456,  1010,  1998,  7372,\n",
       "          1012,  2021,  2130,  2008,  3475,  1521,  1056,  2438,  1012,  2673,\n",
       "          2038,  2042,  2853,  1012,  1037,  2915,  2008, 28837,  1996,  6192,\n",
       "          8164,  2302,  2412,  2437,  3967,  2003,  2053,  2936,  1037,  2416,\n",
       "          1010,  2009,  2003,  1037,  1523,  2748,  2924,  4555,  1524,  1012,\n",
       "          4599,  4995,  1521,  1056,  4599,  1010,  2027,  2024,  1996,  1523,\n",
       "         29536,  2850, 14876,  2638,  5470,  2390,  1524,  1012,  3376,  1010,\n",
       "          3144, 11269, 10107,  2053,  2062, 24931,  2015,  2084,  1523, 10556,\n",
       "         15185,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0114, -0.2594,  0.2148,  ..., -0.0642,  0.6420,  0.6216],\n",
       "         [ 0.3151,  0.0218, -0.2895,  ...,  0.6906,  1.2441,  0.4268],\n",
       "         [ 0.2281,  0.3350,  0.3543,  ...,  0.1682,  0.6898,  0.6343],\n",
       "         ...,\n",
       "         [-0.5402, -0.7427, -0.4525,  ..., -0.0727, -0.3084, -0.6116],\n",
       "         [ 0.4244, -0.3033, -0.1225,  ...,  0.3141, -0.3709, -1.2549],\n",
       "         [-0.0217,  0.0292,  0.3165,  ...,  0.1900, -0.0604, -0.1546]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-0.6571, -0.5322, -0.8875,  0.4373,  0.6194, -0.0529, -0.1200,  0.3075,\n",
       "         -0.6660, -1.0000, -0.2123,  0.8453,  0.9690,  0.5231,  0.6320, -0.2773,\n",
       "          0.3014, -0.6028,  0.3351,  0.7915,  0.6610,  1.0000,  0.1800,  0.3431,\n",
       "          0.4981,  0.8805, -0.4939,  0.8196,  0.8602,  0.7605, -0.0930,  0.3511,\n",
       "         -0.9794, -0.2132, -0.9040, -0.9843,  0.3387, -0.2785,  0.0359, -0.0537,\n",
       "         -0.6841,  0.2893,  1.0000, -0.2314,  0.3175, -0.2279, -1.0000,  0.2127,\n",
       "         -0.5372,  0.7955,  0.6704,  0.8094,  0.2283,  0.4374,  0.3978, -0.5073,\n",
       "         -0.1014,  0.1071, -0.2237, -0.5601, -0.6954,  0.5589, -0.7839, -0.7021,\n",
       "          0.7452,  0.8597, -0.0335, -0.3117, -0.1222,  0.1053,  0.3555,  0.2982,\n",
       "         -0.4134, -0.7882,  0.4857,  0.5088, -0.6788,  1.0000,  0.0906, -0.9483,\n",
       "          0.9114,  0.6685,  0.6077, -0.3236,  0.5196, -1.0000,  0.3927, -0.1541,\n",
       "         -0.9791,  0.3527,  0.4779, -0.2854,  0.8921,  0.5796, -0.3992, -0.5497,\n",
       "         -0.1415, -0.7899, -0.3407, -0.5166,  0.0984, -0.2102, -0.2968, -0.2801,\n",
       "          0.4093, -0.5481,  0.1918,  0.4644,  0.1541,  0.6886,  0.5909, -0.4447,\n",
       "          0.4892, -0.8480,  0.5739, -0.3279, -0.9702, -0.6129, -0.9784,  0.6080,\n",
       "         -0.2109, -0.1575,  0.7512, -0.6777,  0.3982, -0.1993, -0.7695, -1.0000,\n",
       "         -0.1140, -0.5107, -0.1083, -0.4484, -0.9425, -0.9249,  0.5872,  0.8830,\n",
       "          0.2420,  0.9999, -0.2440,  0.8354,  0.1054, -0.5383,  0.3921, -0.5475,\n",
       "          0.5376, -0.8157,  0.0405,  0.2897, -0.1797, -0.1727, -0.6361, -0.2748,\n",
       "         -0.7895, -0.7633, -0.4770,  0.7407, -0.4485, -0.8478,  0.0336, -0.2177,\n",
       "         -0.1284,  0.5080,  0.5284,  0.2882, -0.3758,  0.4324, -0.1755,  0.4402,\n",
       "         -0.6191,  0.1150,  0.3801, -0.4069, -0.8729, -0.9591, -0.3704,  0.3267,\n",
       "          0.9652,  0.5129,  0.3284,  0.4375, -0.3901,  0.4386, -0.9121,  0.9576,\n",
       "         -0.1000,  0.3627, -0.8523,  0.6248, -0.5187,  0.0737,  0.5172, -0.3392,\n",
       "         -0.5995, -0.1872, -0.5532, -0.3273, -0.7391,  0.1597, -0.2336, -0.4398,\n",
       "         -0.2611,  0.8594,  0.5146,  0.1698,  0.1721,  0.5392, -0.5393, -0.1927,\n",
       "          0.2234,  0.2037,  0.1185,  0.9652, -0.7148,  0.0096, -0.7289, -0.9660,\n",
       "         -0.0353, -0.6827, -0.2130, -0.5456,  0.6475, -0.5109,  0.0295,  0.2190,\n",
       "         -0.0246, -0.6312,  0.2953, -0.4770,  0.5679, -0.2834,  0.9830,  0.9260,\n",
       "         -0.5870, -0.5103,  0.9401, -0.9351, -0.6807,  0.1109, -0.2680,  0.4752,\n",
       "         -0.5640,  0.9741,  0.6837,  0.3827, -0.7919, -0.8525, -0.2400, -0.3628,\n",
       "         -0.0410,  0.0488,  0.8183,  0.6636,  0.4182,  0.5722, -0.2231,  0.2424,\n",
       "         -0.9911, -0.9038, -0.9363,  0.0829, -0.9797,  0.8216,  0.3216,  0.3934,\n",
       "         -0.5053, -0.4106, -0.8657,  0.1812,  0.2057,  0.7380, -0.7313, -0.3727,\n",
       "         -0.5097, -0.8302,  0.0518, -0.2222, -0.3776,  0.0274, -0.7431,  0.5945,\n",
       "          0.4560,  0.4504, -0.8644,  0.9530,  1.0000,  0.9142,  0.7389,  0.2537,\n",
       "         -0.9999, -0.8665,  1.0000, -0.9762, -1.0000, -0.7062, -0.4007,  0.1414,\n",
       "         -1.0000, -0.3743,  0.0141, -0.8200,  0.5788,  0.9373,  0.6296, -1.0000,\n",
       "          0.7738,  0.7158, -0.6974,  0.8510, -0.3506,  0.9327,  0.4623,  0.5103,\n",
       "         -0.1598,  0.5597, -0.9367, -0.5942, -0.5473, -0.7153,  0.9990,  0.1892,\n",
       "         -0.6012, -0.6420,  0.5047, -0.1298, -0.0076, -0.8609, -0.2137,  0.1143,\n",
       "          0.4090,  0.1612,  0.2694, -0.1007,  0.2375,  0.2455, -0.1505,  0.6776,\n",
       "         -0.8972, -0.0241,  0.0939, -0.0046, -0.5807, -0.9456,  0.8574, -0.2284,\n",
       "          0.5928,  1.0000,  0.6291, -0.3391,  0.3688,  0.2469, -0.8158,  1.0000,\n",
       "          0.7521, -0.9497, -0.6625,  0.6820, -0.5201, -0.6288,  0.9987, -0.2938,\n",
       "         -0.7414,  0.0278,  0.9715, -0.9815,  0.9960, -0.4617, -0.9236,  0.8481,\n",
       "          0.7935, -0.2981, -0.5695,  0.1825, -0.5197,  0.3299, -0.3741,  0.2898,\n",
       "          0.2489, -0.0395,  0.7181,  0.4557, -0.6603,  0.2846, -0.5296, -0.1912,\n",
       "          0.9217,  0.4546, -0.1363,  0.0244, -0.2811, -0.9275, -0.9039,  0.3863,\n",
       "          1.0000, -0.3722,  0.8673, -0.3475, -0.1233, -0.0692,  0.6444,  0.6396,\n",
       "         -0.2535, -0.6714,  0.7002, -0.3456, -0.9849,  0.1618,  0.2559, -0.1885,\n",
       "          0.9997,  0.3509,  0.3727,  0.4615,  0.9561,  0.0934, -0.1726,  0.8452,\n",
       "          0.9502, -0.2392,  0.6955,  0.0481, -0.8211, -0.2646, -0.6162,  0.1227,\n",
       "         -0.9347,  0.1221, -0.8201,  0.8653,  0.8846,  0.4634,  0.2370,  0.6310,\n",
       "          1.0000, -0.9692,  0.0203,  0.9279, -0.3420, -0.9999, -0.0950, -0.4600,\n",
       "         -0.1224, -0.7851, -0.3484,  0.2456, -0.8885,  0.5796,  0.6614, -0.3382,\n",
       "         -0.9563, -0.5931, -0.2928,  0.1292, -0.9862, -0.3202, -0.4509,  0.3017,\n",
       "         -0.3223, -0.7486,  0.0329, -0.3985,  0.4882, -0.2752,  0.6591,  0.7635,\n",
       "          0.8222, -0.8194, -0.2147, -0.1206, -0.3858,  0.3629, -0.6181, -0.7583,\n",
       "         -0.2921,  1.0000, -0.7170,  0.8858,  0.1671,  0.0444, -0.2530,  0.3094,\n",
       "          0.9263,  0.2939, -0.7106, -0.8329,  0.9326, -0.4419,  0.5046,  0.8204,\n",
       "          0.7023,  0.6706,  0.8284,  0.2062,  0.0018, -0.0076,  0.8631, -0.0856,\n",
       "         -0.2181, -0.2573, -0.1715, -0.3821,  0.8159,  1.0000,  0.2825,  0.4281,\n",
       "         -0.9703, -0.7732, -0.6164,  1.0000,  0.7545, -0.3133,  0.6147,  0.2947,\n",
       "         -0.1840, -0.3050, -0.2047, -0.1849,  0.2965,  0.1766,  0.8574, -0.5753,\n",
       "         -0.9346, -0.5673,  0.3516, -0.8873,  1.0000, -0.5961, -0.3258, -0.1968,\n",
       "         -0.1318, -0.9657,  0.0518, -0.9544, -0.2632,  0.2550,  0.8848,  0.1907,\n",
       "         -0.6778, -0.6261,  0.4937,  0.4257, -0.7750, -0.8479,  0.8940, -0.8653,\n",
       "          0.4686,  1.0000,  0.4111, -0.0140,  0.1316, -0.0947,  0.4341, -0.2486,\n",
       "          0.2793, -0.8363, -0.4057, -0.2214,  0.2854, -0.2285, -0.6055,  0.3593,\n",
       "          0.2725, -0.6417, -0.6352, -0.0656,  0.3925,  0.6594, -0.2816, -0.0745,\n",
       "          0.0348, -0.0216, -0.7940, -0.4896, -0.3780, -1.0000,  0.4117, -1.0000,\n",
       "          0.4358,  0.0579, -0.2935,  0.6432,  0.8416,  0.7880, -0.2932, -0.8320,\n",
       "          0.5211,  0.6298, -0.3520,  0.1702, -0.1709,  0.3683, -0.0439,  0.2047,\n",
       "         -0.4598,  0.6502, -0.1889,  1.0000,  0.2444, -0.1657, -0.2702,  0.3077,\n",
       "         -0.1912,  1.0000,  0.0535, -0.8930,  0.3159, -0.5776, -0.6287,  0.4564,\n",
       "         -0.0066, -0.4227, -0.8717,  0.6588, -0.0704, -0.6672,  0.5659, -0.3129,\n",
       "         -0.3128,  0.1605,  0.8491,  0.9662,  0.5859,  0.5382, -0.7554, -0.6407,\n",
       "          0.9223,  0.2658, -0.4931,  0.2261,  1.0000,  0.2974, -0.7615,  0.0804,\n",
       "         -0.7937, -0.1780, -0.7566,  0.2509,  0.2143,  0.8169, -0.2629,  0.8351,\n",
       "         -0.8092, -0.0395,  0.0557, -0.4006,  0.3231, -0.6278, -0.9617, -0.9634,\n",
       "          0.5180, -0.5013, -0.1330,  0.4326,  0.2538,  0.4388,  0.4356, -1.0000,\n",
       "          0.8585,  0.4336,  0.8023,  0.8828,  0.7512,  0.5143,  0.3468, -0.9467,\n",
       "         -0.3405, -0.2911, -0.3788,  0.4333,  0.6587,  0.7416,  0.4134, -0.4905,\n",
       "         -0.6853, -0.5234, -0.9656, -0.9813,  0.5033, -0.5310, -0.0407,  0.9020,\n",
       "         -0.0272, -0.1757, -0.1688, -0.7711, -0.0077,  0.5521, -0.1457,  0.0523,\n",
       "          0.3091,  0.6466,  0.6477,  0.9675, -0.8200,  0.1655, -0.6043,  0.4441,\n",
       "          0.9563, -0.9137,  0.2092,  0.4474, -0.2849,  0.3964, -0.4428, -0.5106,\n",
       "          0.8301, -0.3115,  0.3467, -0.3729,  0.0277, -0.4171, -0.2615, -0.5584,\n",
       "         -0.2911,  0.6654, -0.2867,  0.6969,  0.8048, -0.1302, -0.4000, -0.1795,\n",
       "         -0.5958, -0.8068,  0.0643, -0.0183, -0.0419,  0.7326, -0.0918,  0.9698,\n",
       "          0.0625, -0.4157, -0.2614, -0.5470,  0.6367, -0.5815, -0.5862, -0.2848,\n",
       "          0.4576,  0.2337,  1.0000, -0.7244, -0.8347, -0.4185, -0.3842,  0.4898,\n",
       "         -0.0793, -1.0000,  0.2948, -0.1967,  0.7345, -0.5090,  0.7857, -0.3228,\n",
       "         -0.7936, -0.3774,  0.7536,  0.6134, -0.5430, -0.3767,  0.6235, -0.1955,\n",
       "          0.8710,  0.5066,  0.3329,  0.2806,  0.6826, -0.7026, -0.5607,  0.5834]],\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 762/762 [00:00<?, ?B/s] \n",
      "c:\\Users\\GaneshKarbhari\\.conda\\envs\\summ_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\GaneshKarbhari\\.cache\\huggingface\\hub\\models--google-bert--bert-large-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "model.safetensors: 100%|██████████| 1.34G/1.34G [04:24<00:00, 5.07MB/s]\n",
      "Some weights of the model checkpoint at google-bert/bert-large-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "tokenizer_config.json: 100%|██████████| 49.0/49.0 [00:00<00:00, 43.8kB/s]\n",
      "vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 2.31MB/s]\n",
      "tokenizer.json: 100%|██████████| 436k/436k [00:00<00:00, 1.87MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"fill-mask\", model=\"google-bert/bert-large-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.06813004612922668,\n",
       "  'token': 2880,\n",
       "  'token_str': 'foreign',\n",
       "  'sequence': 'Ronaldo is the best foreign player.'},\n",
       " {'score': 0.05797560513019562,\n",
       "  'token': 1835,\n",
       "  'token_str': 'international',\n",
       "  'sequence': 'Ronaldo is the best international player.'},\n",
       " {'score': 0.04619794338941574,\n",
       "  'token': 1227,\n",
       "  'token_str': 'known',\n",
       "  'sequence': 'Ronaldo is the best known player.'},\n",
       " {'score': 0.04542800039052963,\n",
       "  'token': 5468,\n",
       "  'token_str': 'Brazilian',\n",
       "  'sequence': 'Ronaldo is the best Brazilian player.'},\n",
       " {'score': 0.04050658643245697,\n",
       "  'token': 1735,\n",
       "  'token_str': 'European',\n",
       "  'sequence': 'Ronaldo is the best European player.'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"Ronaldo is the best [MASK] player.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BatchEncoding.words of {'input_ids': tensor([[  101,  1045,  2113,  2008,  2144,  1996, 12149,  1997,  1996,  2322,\n",
       "          1011,  2058,  4289,  1037,  5476,  1998,  1037,  2431,  2067,  1010,\n",
       "          1998,  2008,  1997,  1996,  2796,  4239,  2223,  1006, 12997,  2140,\n",
       "          1007,  1018,  2086,  2044,  2008,  1010,  1996, 20047,  2433,  1997,\n",
       "          1996,  2208,  2003,  1996,  2087,  2759,  2028,  1012,  2009,  2038,\n",
       "          2036,  2589, 16278,  2005,  9716,  1996, 17763,  2006,  1996,  9046,\n",
       "          5848,  2013,  2536,  3032,  1012,  1998,  2633,  1010,  1996,  2223,\n",
       "          4289,  2038,  2036, 18449,  1996,  2120,  7372,  2011,  2383,  2780,\n",
       "          5398,  1997,  2867,  2013,  2536, 24904,  1012,  2021,  1045,  2228,\n",
       "          2008,  2003,  1996,  2203,  1997,  1523,  2673,  2204,  2055,  1056,\n",
       "         11387,  4533,  1998,  1996, 12997,  2140,  1524,  1012,  1045,  3427,\n",
       "          1996,  2034,  1016,  3692,  1997,  2023,  4533,  4469,  3567,  5289,\n",
       "          4143,  1010,  2021,  1045,  3432,  2064,  1521,  1056,  4562,  2009,\n",
       "          2085,  1012,  2009,  2053,  2936,  5683,  2066,  4533,  1012,  2009,\n",
       "          2987,  1521,  1056,  2130,  2514,  2066,  1037,  2613,  4368,  4902,\n",
       "          1012,  2045,  2003,  2053,  2645,  1997, 25433,  1012,  2053,  5049,\n",
       "          1997,  5848,  1012,  2053,  4653,  1997,  2995,  6143, 11067,  1012,\n",
       "          2009,  1521,  1055,  2035,  1037,  2265,  1012,  4024,  1012,  1996,\n",
       "         12997,  2140,  2003,  2000,  4533,  2054, 11700,  2003,  2000,  4843,\n",
       "          1011,  5189,  1010,  4118,  7476,  1010,  1998,  5675,  2594,  1012,\n",
       "          2009,  1521,  1055,  1037, 12236,  3549,  1521,  1055,  8917,  2100,\n",
       "          1012,  1037, 11320, 14808, 13288,  4653,  1997,  3928,  1010,  6228,\n",
       "          7171,  1012,  1037,  1017,  3178,  2146, 23667, 14081,  1997,  2040,\n",
       "          2064, 18365,  1996,  6493,  6192,  1011,  2053,  8066,  1010,  2053,\n",
       "         21892,  3366,  1010,  2074,  5760,  6740,  1012,  1996,  4306,  2003,\n",
       "          2061,  2757,  2008,  2027,  2342,  2000, 15712,  2008, 15703, 12997,\n",
       "          2140,  2299,  2296,  2048, 15849,  1012,  1998,  2059,  8479,  3168,\n",
       "          3238, 16046, 10299,  2000, 26136,  6593, 20598,  1996,  4223,  1999,\n",
       "          1996,  3346,  1012,  2633,  1010,  2045,  2003,  2008, 15703,  1054,\n",
       "          3501,  2711,  2040,  2038,  2000, 10083,  2009,  3458, 12611,  2007,\n",
       "          2037,  5048,  2100,  7928,  1010,  2877, 29534,  1010,  1998,  3452,\n",
       "         29348, 16115,  1012,  1045,  2812,  2065,  1996,  2208,  2003,  2204,\n",
       "          1010,  2017,  2876,  1521,  1056,  2342,  2035,  2122, 13675,  4904,\n",
       "          8376,  1012,  2017,  2123,  1521,  1056,  2342,  1037, 24716,  5189,\n",
       "         13102, 25508,  2121,  2376,  2000,  2707,  1037,  1523, 28144, 10698,\n",
       "         28144, 10698,  1524, 16883,  1012, 17266, 10606,  2196,  2734,  2008,\n",
       "          1012,  4445,  2106,  7367, 18663,  2290,  1012,  1998,  2059,  3310,\n",
       "          1996, 10866,  5971, 22040,  1997,  1996,  4647,  6895,  1012,  2017,\n",
       "          2031,  2115,  7281,  4341,  1010,  2115, 14389,  1010,  6599,  2013,\n",
       "          1996,  2358, 25205,  1998,  1996,  2334,  4533,  4230,  1010,  1996,\n",
       "         12026,  2015,  2013,  2536,  3316,  2040, 10460,  1996,  2536,  2695,\n",
       "          1011,  2674,  1998,  2695,  1011,  2186,  2982,  1010,  1998,  1996,\n",
       "         10866,  3616,  1997, 20380,  2015,  2006,  2115,  2867,  1521, 26278,\n",
       "          1010, 11180,  4908,  2015,  1010,  4533, 19456,  1010,  1998,  7372,\n",
       "          1012,  2021,  2130,  2008,  3475,  1521,  1056,  2438,  1012,  2673,\n",
       "          2038,  2042,  2853,  1012,  1037,  2915,  2008, 28837,  1996,  6192,\n",
       "          8164,  2302,  2412,  2437,  3967,  2003,  2053,  2936,  1037,  2416,\n",
       "          1010,  2009,  2003,  1037,  1523,  2748,  2924,  4555,  1524,  1012,\n",
       "          4599,  4995,  1521,  1056,  4599,  1010,  2027,  2024,  1996,  1523,\n",
       "         29536,  2850, 14876,  2638,  5470,  2390,  1524,  1012,  3376,  1010,\n",
       "          3144, 11269, 10107,  2053,  2062, 24931,  2015,  2084,  1523, 10556,\n",
       "         15185,  2239,  2078, 27829, 11057,  2140,  1524,  1012,  2021,  2130,\n",
       "          2065,  2017,  8568,  1996, 13675, 23496, 29290,  1010, 17503,  5971,\n",
       "         20380,  2015,  4857,  2135, 15562,  2098,  2004,  8570,  1010,  2008,\n",
       "          3327,  2396,  2433,  2003,  2145,  1999,  1996,  2197, 16215,  3217,\n",
       "          2229,  1997,  2049,  7554,  2166,  1012,  1045,  2001,  2074,  3666,\n",
       "          1996, 16920,  5443, 13624,  2674,  1998, 28144, 10698,  2001,  2320,\n",
       "          2062,  1996,  6706,  8133,  2005,  2010,  2136,  1012,  2002,  2001,\n",
       "          2667,  1010,  2004,  2002,  2515,  1010,  2000,  5703,  2119, 14974,\n",
       "          1998,  1041, 16211,  3490, 16383,  1006,  1998,  2725,  1010,  2004,\n",
       "          3517,  1010,  1037, 17227,  3105,  1007,  1012,  1996, 12268,  2020,\n",
       "          2667,  2000, 17908,  2010,  2433,  1010,  2010,  2806,  1010,  1998,\n",
       "          2667,  2000, 11703, 11514,  5886,  2054,  2003,  2183,  2006,  1999,\n",
       "          2008, 14021, 15603,  2094,  2568,  1997,  2010,  1012,  2021,  2035,\n",
       "          2027,  2071, 20327,  2001,  1523,  2002,  2003,  2107,  1037,  2844,\n",
       "          2447,  1524,  1010,  1523, 28144, 10698,  2038,  2107,  2844, 27323,\n",
       "          1524,  1010,  1998,  1523,  2010,  3356,  2303,  3997,  2003,  9788,\n",
       "          1524,  1012,  2008,  1521,  1055,  2074, 13971,  1010, 11771,  1010,\n",
       "          1998, 19597,  5805,  2000,  1996,  4599,  1012,  3071,  4282,  2008,\n",
       "          1012,  2074,  2066,  3071,  4282,  2008,  1996, 11777,  2136,  3791,\n",
       "          2000,  3556,  3216,  1998,  3828, 10370,  2000,  2663,  1012,  2030,\n",
       "          2008,  2028, 16775,  2058,  2071,  6297,  1996, 12677,  2005,  1996,\n",
       "          6984,  2217,  1012,  2073,  2024,  1996, 14742, 21864,  4523,  1029,\n",
       "          4248, 16371, 13871,  8454,  1997,  4533,  2075,  5656,  1029,  3441,\n",
       "          2055,  1996,  2208,  1998,  1996,  2867,  1029,  2123,  1521,  1056,\n",
       "          2425,  2033,  2008,  2009,  2001,  1037,  1523,  2092, 10849,  2301,\n",
       "          1524,  1025,  9603,  2006,  2339,  2009,  2001,  1012,  2123,  1521,\n",
       "          1056,  2655,  2296,  2416,  1523, 12047,  1524,  1010,  2030,  2296,\n",
       "          4550, 19831,  6959,  1037,  1523, 24646, 10087,  2099,  1524,  1012,\n",
       "          2123,  1521,  1056,  3191,  2041,  1996,  6747,  2008,  2024,  2006,\n",
       "          1996,  3898,  2005,  3071,  2000,  3191,  1012,  2175,  3191,  2055,\n",
       "          2023,  4368,  2008,  2003,  2085,  1037,  9575,  1012,  2113,  2049,\n",
       "          2381,  1010,  2049,  2671,  1010,  2049,  3906,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 49.0/49.0 [00:00<?, ?B/s]\n",
      "c:\\Users\\GaneshKarbhari\\.conda\\envs\\summ_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\GaneshKarbhari\\.cache\\huggingface\\hub\\models--bert-base-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 562kB/s]\n",
      "tokenizer.json: 100%|██████████| 436k/436k [00:00<00:00, 692kB/s]\n",
      "config.json: 100%|██████████| 570/570 [00:00<?, ?B/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Natural', 'Science', 'Museum', 'of', 'Madrid', 'shows', 'the', 'R', '##EC', '##ON', '##ST', '##R', '##UC', '##TI', '##ON', 'of', 'a', 'dinosaur']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tz = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "sentence = \"The Natural Science Museum of Madrid shows the RECONSTRUCTION of a dinosaur\"\n",
    "\n",
    "tokens = tz.tokenize(sentence)\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summ_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
